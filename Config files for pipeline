# Logstash → Fluentd → PostgreSQL/TimescaleDB Pipeline

## Overview
This project implements a complete log ingestion pipeline for Microsoft IIS logs. The pipeline parses raw IIS log files, normalizes fields, stores them in a PostgreSQL 15 database (extended with TimescaleDB for time-series capabilities), and makes the data available for visualization in Grafana.  

The project was completed as part of an internship, with the following goals:
- Build a robust, reproducible pipeline from Logstash → Fluentd → PostgreSQL/TimescaleDB.  
- Parse IIS log fields accurately (datetime, IP, cookies, URIs, status codes, response times, etc.).  
- Handle failures gracefully.  
- Validate correctness (no grokparsefailures, exact record counts, accurate cookie parsing).  
- Provide documentation and operational instructions for long-term use.  

---

## Architecture
The pipeline follows this flow:  

**Logstash → Fluentd → PostgreSQL (TimescaleDB) → Grafana**  

- **Logstash** parses raw IIS logs, extracts required fields, classifies logs (success, static content, failures), and sends records over HTTP to Fluentd.  
- **Fluentd** receives logs, applies lightweight transforms, and bulk-inserts records into PostgreSQL.  
- **PostgreSQL with TimescaleDB** stores logs in partitioned hypertables for efficient time-series queries.  
- **Grafana** visualizes metrics such as error rates, traffic volumes, response times, and static vs. dynamic content.  

---

## Setup

### Logstash
- Config directory: `~/logstash_configs/`  
- Start Logstash:  
  ```bash
  sudo /usr/share/logstash/bin/logstash -f ~/logstash_configs/logstash.conf
  ```  
- Stop Logstash:  
  ```bash
  pkill -f logstash    # or Ctrl+C if running in foreground
  ```  

### Fluentd
- Config directory: `~/fluentd_conf/`  
- Start Fluentd:  
  ```bash
  fluentd --config ~/fluentd_conf/fluent.conf
  ```  
- Stop Fluentd:  
  ```bash
  pkill -f fluentd     # or Ctrl+C if running in foreground
  ```  

### PostgreSQL + TimescaleDB
- Access database:  
  ```bash
  sudo -u postgres psql -d iis_logs
  ```  

### Grafana
- Run SSH tunnel (on your local machine, outside the VM):  
  ```bash
  ssh -L 3001:localhost:3000 -p 2212 aaronbadmin@dcir-sandbox7.domain-msi.local
  ```  
- Access Grafana in browser:  
  ```
  http://localhost:3001/
  ```  

---

## Configurations

### Logstash Configuration (`~/logstash_configs/logstash.conf`)

```conf
input {
  file {
    # Two IIS log sources
    path => [
      "/home/aaronbadmin@domain-msi.local/iis_logs/intranet/u_ex250414_x.log",
      "/home/aaronbadmin@domain-msi.local/iis_logs/tablet/u_ex250414_x.log"
    ]
    start_position => "beginning"
    sincedb_path => "/dev/null"   # ensures full reread on restart
  }
}

filter {
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:v_datetime} %{IPORHOST:c_ip} %{DATA:cs_method} %{URIPATH:cs_uri_stem} %{URIPARAM:cs_uri_query}? %{NUMBER:sc_status} %{NUMBER:sc_bytes} %{NUMBER:cs_bytes} %{NUMBER:time_taken}"
    }
    tag_on_failure => ["_grokparsefailure"]
  }

  # Default values for failed parses
  if "_grokparsefailure" in [tags] {
    mutate {
      add_field => { "reason" => "grokparsefailure" }
    }
  }

  # Parse cookies (extract teammsiuid as cs_username)
  kv {
    source => "cs_cookie"
    field_split => ";"
    value_split => "="
  }
  if [teammsiuid] {
    mutate {
      rename => { "teammsiuid" => "cs_username" }
    }
  }

  # Normalize datetime
  date {
    match => ["v_datetime", "YYYY-MM-dd HH:mm:ss"]
    target => "@timestamp"
  }

  # Drop raw message
  mutate {
    remove_field => ["message"]
  }

  # Static content classification
  if [cs_uri_stem] =~ /\.(css|js|png|jpg|gif|ico)$/ {
    mutate { add_field => { "record_type" => "static" } }
  } else {
    mutate { add_field => { "record_type" => "dynamic" } }
  }
}

output {
  # Main logs
  http {
    url => "http://localhost:9880/http.logs"
    http_method => "post"
    format => "json"
  }

  # Failed parses
  if "_grokparsefailure" in [tags] {
    http {
      url => "http://localhost:9880/http.log_failures"
      http_method => "post"
      format => "json"
    }
  }

  # Static content logs
  if [record_type] == "static" {
    http {
      url => "http://localhost:9880/http.static"
      http_method => "post"
      format => "json"
    }
  }
}
```

---

### Fluentd Configuration (`~/fluentd_conf/fluent.conf`)

```conf
<source>
  @type http
  port 9880
</source>

# Normal logs
<match http.logs>
  @type postgres_bulk
  host localhost
  database iis_logs
  username postgres
  password postgres
  table iis_log_records
  column_names v_datetime,c_ip,cs_cookie,cs_host,cs_referer,cs_uri_query,cs_uri_stem,cs_username,s_computername,sc_status,time_taken,sc_bytes,cs_bytes
  <buffer>
    flush_interval 2s
    chunk_limit_size 16MB
  </buffer>
</match>

# Failed parses
<match http.log_failures>
  @type postgres_bulk
  host localhost
  database iis_logs
  username postgres
  password postgres
  table iis_log_failures
  column_names reason,raw_log
  <buffer>
    flush_interval 2s
  </buffer>
</match>

# Static content logs
<match http.static>
  @type postgres_bulk
  host localhost
  database iis_logs
  username postgres
  password postgres
  table iis_logs_static
  column_names v_datetime,cs_username,cs_uri_stem
  <buffer>
    flush_interval 2s
  </buffer>
</match>
```

---

## Database Schema

```sql
-- Main log table
CREATE TABLE iis_log_records (
    v_datetime timestamptz NOT NULL,
    c_ip inet,
    cs_cookie text,
    cs_host text,
    cs_referer text,
    cs_uri_query text,
    cs_uri_stem text,
    cs_username text,
    s_computername text,
    sc_status integer,
    time_taken integer,
    sc_bytes bigint,
    cs_bytes bigint
);
CREATE INDEX ON iis_log_records (v_datetime DESC);
SELECT create_hypertable('iis_log_records', 'v_datetime');

-- Static content table
CREATE TABLE iis_logs_static (
    v_datetime timestamptz,
    cs_username text,
    cs_uri_stem text
);
CREATE INDEX ON iis_logs_static (v_datetime);
CREATE INDEX ON iis_logs_static (cs_uri_stem);
CREATE INDEX ON iis_logs_static (cs_username);

-- Failure table
CREATE TABLE iis_log_failures (
    id serial PRIMARY KEY,
    failure_time timestamptz DEFAULT now(),
    reason text,
    raw_log text
);
```

---

## Operations Guide

### Config Files
- View Logstash config:  
  ```bash
  cat ~/logstash_configs/logstash.conf
  ```  
- Edit Logstash config:  
  ```bash
  nano ~/logstash_configs/logstash.conf
  ```  
- View Fluentd config:  
  ```bash
  cat ~/fluentd_conf/fluent.conf
  ```  
- Edit Fluentd config:  
  ```bash
  nano ~/fluentd_conf/fluent.conf
  ```  

### Process Management
- Check Logstash process:  
  ```bash
  ps aux | grep logstash
  ```  
- Check Fluentd process:  
  ```bash
  ps aux | grep fluentd
  ```  

### CSV Exports
- Export to local machine:  
  ```bash
  scp -P 2212 aaronbadmin@dcir-sandbox7.domain-msi.local:/tmp/final_normalized_cs_uri_stem_v2.csv .
  ```  
- Open in file explorer:  
  ```bash
  explorer .
  ```  

---

## Validation

1. **Successful Parsing**  
   - All 3.13M log lines were parsed with **zero grokparsefailures**.  
   - Verified with SQL queries against `iis_log_failures` and by grepping Logstash logs.  

2. **Record Counts**  
   - PostgreSQL record counts exactly matched raw log line counts.  

3. **Cookie Parsing**  
   - `teammsiuid` correctly extracted into `cs_username`.  
   - Verified using SQL queries and raw log sampling.  

4. **Static vs. Dynamic Classification**  
   - Static assets (CSS/JS/images) routed to `iis_logs_static`.  
   - Dynamic requests stored in `iis_log_records`.  

---

## Conclusion
This project successfully delivers a fully operational IIS log ingestion pipeline:  

- **Logstash** parses and enriches raw logs.  
- **Fluentd** efficiently bulk-inserts into PostgreSQL.  
- **PostgreSQL with TimescaleDB** provides scalable storage and indexing.  
- **Grafana** enables visualization of server metrics.  

The pipeline was rebuilt on a fresh VM, verified with multi-million line IIS logs, and produced exact count matches with no parsing errors. It is production-ready and extensible for future analytics, dashboards, and monitoring use cases.  
